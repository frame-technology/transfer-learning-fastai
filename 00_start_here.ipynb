{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Transfer Learning Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to [frame.ai's](https://frame.ai) exploration of transfer learning with [fast.ai](https://github.com/fastai/fastai) and [floydhub](https://www.floydhub.com). We assume you are viewing this on a pytorch 1.0 machine on floydhub. If you are instead viewing this notebook locally, please make sure you have pytorch 1.0 installed and the remainder of `setup.sh` and `floyd_requirements.txt` installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Floydhub Jobs\n",
    "\n",
    "If all you want to do is kick off jobs on floydhub all you need to do is to run the following two cells. If you are running this on floydhub, you should be good to go. If you are running locally, just make sure you have the floydhub CLI installed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running samples larger than 1000 please use --gpu2 instead of --cpu\n",
    "def train_grid(exp_name, sample_sizes):\n",
    "    for size in sample_sizes:\n",
    "        for global_lm in [True, False]:\n",
    "            !floyd run \"bash setup.sh && python train.py mytest floyd \\\n",
    "                --sample-size={size} --global-lm={global_lm}\" \\\n",
    "                --env pytorch-1.0 --cpu \\\n",
    "                -m \"sample size {size}, global_lm {global_lm}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in our experiment we ran sample_sizes of [500, 1000, 2000, 4000, 8000, 16000, 32000, 64000]\n",
    "train_grid('frame_blog_experiment', sample_sizes=[500, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Playground\n",
    "If you'd like to play around with fast.ai and the experiment here you can do so below. \n",
    "\n",
    "**Please note** that even the smallest sample sizes are a lot for a non-GPU machine to handle. Our advice is to not attempt anything above 1000 domain samples locally, and anything above 16000 samples on a machine not equivalent or better than a floydhub `GPU2` machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 500\n",
    "exp_name = 'frame_blog_experiment'\n",
    "env = 'local'\n",
    "global_lm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15732f91e9ac45349479a60f42dc29b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=221972701), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c905577b30a148beb68ef9af5f010236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1027972), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = '_'.join([exp_name, str(sample_size)])\n",
    "data_dir = Path(f'./{data_dir}/')\n",
    "\n",
    "# grab and parse imdb review sentiment data\n",
    "df_trn, df_val = get_imdb_data(data_dir)\n",
    "\n",
    "# make sure we have wikitext language model\n",
    "model_path = data_dir / 'models'\n",
    "model_path.mkdir(exist_ok=True)\n",
    "url = 'http://files.fast.ai/models/wt103_v1/'\n",
    "download_url(f'{url}lstm_wt103.pth', model_path / 'lstm_wt103.pth')\n",
    "download_url(f'{url}itos_wt103.pkl', model_path / 'itos_wt103.pkl')\n",
    "\n",
    "# create csv samples to feed into fast.ai \n",
    "sample_for_experiment(train_df=df_trn,\n",
    "                      test_df=df_val,\n",
    "                      sample_size=sample_size,\n",
    "                      dst=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Language Model...\n",
      "Tokenizing train_lm.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing train_lm.\n",
      "Tokenizing valid.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing valid.\n",
      "LM Train Vocabulary size: 3986\n",
      "LM Vocabulary size: 3986\n",
      "LM Embedding dim: 400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=8), HTML(value='0.00% [0/8 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 41:18\n",
      "epoch  train loss  valid loss  accuracy\n",
      "1      4.847859    4.075024    0.247282  (05:06)\n",
      "2      4.613337    3.959728    0.255174  (04:59)\n",
      "3      4.441497    3.936216    0.257463  (04:52)\n",
      "4      4.307472    3.934896    0.257945  (05:10)\n",
      "5      4.184432    3.936925    0.257398  (05:20)\n",
      "6      4.082025    3.942329    0.258600  (05:06)\n",
      "7      3.974310    3.955752    0.257507  (04:58)\n",
      "8      3.876446    3.971110    0.255521  (05:42)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Language Model...\")\n",
    "lm_encoder_name = 'lm1_enc'\n",
    "lm_learner, lm_data = train_language_model(\n",
    "    data_dir, env, global_lm)\n",
    "lm_learner.save_encoder(lm_encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sentiment Classifier...\n",
      "Tokenizing train_clas.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing train_clas.\n",
      "Classifier Train Data Vocabulary size: 3986\n",
      "Classifier Vocabulary size: 3986\n",
      "Classifier Embedding dim: 400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=8), HTML(value='0.00% [0/8 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 21:55\n",
      "epoch  train loss  valid loss  accuracy\n",
      "1      0.644545    0.674134    0.534000  (02:33)\n",
      "2      0.586209    0.612849    0.708000  (02:44)\n",
      "3      0.553098    0.536815    0.744000  (02:41)\n",
      "4      0.529575    0.503825    0.772000  (03:08)\n",
      "5      0.492469    0.498575    0.756000  (02:31)\n",
      "6      0.469248    0.490786    0.752000  (02:53)\n",
      "7      0.450089    0.489501    0.758000  (02:39)\n",
      "8      0.432364    0.496952    0.756000  (02:43)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.5340000014305115'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training Sentiment Classifier...\")\n",
    "sentiment_learner = train_classification_model(\n",
    "    data_dir, env, lm_data, lm_encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
